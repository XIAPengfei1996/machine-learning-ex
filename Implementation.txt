%%%%%%% Programming Assignment
%%%%%%% Machine Learning - Stanford University | Coursera
%
%
%%%%%   Working on and Submitting Programming Assignments
% https://www.coursera.org/learn/machine-learning/lecture/Lt2Mx/working-on-and-submitting-programming-assignments
%
%



%%%   Linear Regression

octave:1> PS1('>> ')
>> pwd
ans = /Users/xiapengfei
>> cd /Users/xiapengfei/Desktop/machine-learning-ex1/ex1
>> % addpath('/Users/xiapengfei/Desktop/machine-learning-ex1/ex1’) % Can’t load!
>> warmUpExercise()
ans =

Diagonal Matrix

   1   0   0   0   0
   0   1   0   0   0
   0   0   1   0   0
   0   0   0   1   0
   0   0   0   0   1

>> 
>> X = data(:, 1); y = data(:, 2);
>> m = length(y); % number of training examples
>> plotData(X, y)
>> 
>> X = [ones(m, 1), data(:,1)]; % Add a column of ones to x
>> theta = zeros(2, 1); % initialize fitting parameters
>> iterations = 1500;
>> alpha = 0.01;
>> J = computeCost(X, y, theta)
J =  32.073
>> 
>> predict1 = [1, 3.5] * theta;
>> predict2 = [1, 7] * theta;
>> theta = gradientDescent(X, y, theta, alpha, iterations)
theta =

  -3.6303
   1.1664

>> predict1 = [1, 3.5] * theta;
>> predict2 = [1, 7] * theta;
>> 
>> ex1



Running warmUpExercise ...
5x5 Identity Matrix:
ans =

Diagonal Matrix

   1   0   0   0   0
   0   1   0   0   0
   0   0   1   0   0
   0   0   0   1   0
   0   0   0   0   1

Program paused. Press enter to continue.
Plotting Data ...
Program paused. Press enter to continue.

Testing the cost function ...
With theta = [0 ; 0]
Cost computed = 32.072734
Expected cost value (approx) 32.07

With theta = [-1 ; 2]
Cost computed = 54.242455
Expected cost value (approx) 54.24
Program paused. Press enter to continue.

Running Gradient Descent ...
Theta found by gradient descent:
-3.630291
1.166362
Expected theta values (approx)
 -3.6303
  1.1664

For population = 35,000, we predict a profit of 4519.767868
For population = 70,000, we predict a profit of 45342.450129
Program paused. Press enter to continue.
Visualizing J(theta_0, theta_1) ...
>> 
>> 
>> 
>> data = load('ex1data2.txt');
>> X = data(:, 1:2);
>> y = data(:, 3);
>> m = length(y);
>> [X mu sigma] = featureNormalize(X);
warning: operator -: automatic broadcasting operation applied
warning: quotient: automatic broadcasting operation applied
>> X = [ones(m, 1) X];
>> 
>> alpha = 0.3;
>> num_iters = 400;
>> theta = zeros(3, 1);
>> [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters);
>> theta
theta =

   3.3430e+05
   1.0009e+05
   3.6735e+03

>> plot(1:numel(J_history), J_history, '-b', 'LineWidth', 2);
>> xlabel('Number of iterations');
>> ylabel('Cost J');
>> 
>> alpha1 = 2; theta1 = zeros(3, 1);
>> [theta1, J1] = gradientDescentMulti(X, y, theta1, alpha1, num_iters);
>> alpha2 = 1; theta2 = zeros(3, 1);
>> [theta2, J2] = gradientDescentMulti(X, y, theta2, alpha2, num_iters);
>> alpha3 = 0.1; theta3 = zeros(3, 1);
>> [theta3, J3] = gradientDescentMulti(X, y, theta3, alpha3, num_iters);
>> plot(1:50, J1(1:50), 'g', 'LineWidth', 2);
>> hold on;
>> plot(1:50, J2(1:50), 'r', 'LineWidth', 2);
>> plot(1:50, J3(1:50), 'k', 'LineWidth', 2);
>> plot(1:numel(J_history), J_history, '-b', 'LineWidth', 2);
>> axis([0 50 0 10^11])
>> xlabel('Number of iterations');
>> ylabel('Cost J');
>> legend('α=2','α=1','α=0.1','α=0.01');
>> title('Selecting learning rates');
>> % print -dpng 'Selecting.png';
warning: print.m: fig2dev binary is not available.
Some output formats are not available.
Fontconfig warning: ignoring UTF-8: not a valid region tag
>> sqrt = ((1650 - mean(X(:, 2))) ./ std(X(:, 2)));
>> br = ((3 - mean(X(:, 3))) ./ std(X(:, 3)));
>> price =[1, sqrt, br] * theta2;
>> price
price =    2.9308e+05
>> 
>> data = csvread('ex1data2.txt');
>> X = data(:, 1:2);
>> y = data(:, 3);
>> m = length(y);
>> X = [ones(m, 1) X];
>> theta = normalEqn(X, y)
>> price =[1, 1650, 3] * theta
price =    2.9308e+05
>> 
>> ex1_multi

Loading data ...
First 10 examples from the dataset: 
 x = [2104 3], y = 399900 
 x = [1600 3], y = 329900 
 x = [2400 3], y = 369000 
 x = [1416 2], y = 232000 
 x = [3000 4], y = 539900 
 x = [1985 4], y = 299900 
 x = [1534 3], y = 314900 
 x = [1427 3], y = 198999 
 x = [1380 3], y = 212000 
 x = [1494 3], y = 242500 
Program paused. Press enter to continue.
Normalizing Features ...
warning: operator -: automatic broadcasting operation applied
warning: quotient: automatic broadcasting operation applied
Running gradient descent ...
Theta computed from gradient descent: 
 334302.063993 
 100087.116006 
 3673.548451 

Predicted price of a 1650 sq-ft, 3 br house (using gradient descent):
 $289314.620338
Program paused. Press enter to continue.
Solving with normal equations...
Theta computed from the normal equations: 
 89597.909542 
 139.210674 
 -8738.019112 

Predicted price of a 1650 sq-ft, 3 br house (using normal equations):
 $293081.464335
>> 
Predicted price of a 1650 sq-ft, 3 br house (using normal equations):
 $293081.464335
>> 
>> 
>> 
>> submit()
== Submitting solutions | Linear Regression with Multiple Variables...
Use token from last successful submission (U201412842@hust.edu.cn)? (Y/n): Y
warning: operator -: automatic broadcasting operation applied
warning: quotient: automatic broadcasting operation applied
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==                            Warm-up Exercise |  10 /  10 | Nice work!
==           Computing Cost (for One Variable) |  40 /  40 | Nice work!
==         Gradient Descent (for One Variable) |  50 /  50 | Nice work!
==                       Feature Normalization |   0 /   0 | Nice work!
==     Computing Cost (for Multiple Variables) |   0 /   0 | Nice work!
==   Gradient Descent (for Multiple Variables) |   0 /   0 | Nice work!
==                            Normal Equations |   0 /   0 | Nice work!
==                                   --------------------------------
==                                             | 100 / 100 | 
== 



%%%   Logistic Regression

octave:1> PS1('>> ')
>> cd /Users/xiapengfei/Desktop/machine-learning-ex2/ex2
>> data = load('ex2data1.txt');
>> X = data(:, [1, 2]); y = data(:, 3);
>> % Find Indices of Positive and Negative Examples
>> pos = find(y == 1); neg = find(y == 0);
>> % Plot Examples  %% NO comment in your .m file code
>> plot(X(pos, 1), X(pos, 2), 'k+','LineWidth', 2,'MarkerSize', 7);
>> hold on;
>> plot(X(neg, 1), X(neg, 2), 'ko', 'MarkerFaceColor', 'y','MarkerSize', 7);
>> xlabel('Exam 1 score')
>> ylabel('Exam 2 score')
>> legend('Admitted', 'Not admitted')
>> hold off;
>> 
>> [m, n] = size(X);
>> X = [ones(m, 1) X];
>> initial_theta = zeros(n + 1, 1);
>> h = sigmoid(X * initial_theta);
>> J = (-1 /m ) * (y' * log(h) + (1 - y)' * log(1 - h));
>> grad = (1 / m) * X' * (h - y);
>> test_theta = [-24; 0.2; 0.2];
>> h = sigmoid(X * test_theta);
>> J = (-1 /m ) * (y' * log(h) + (1 - y)' * log(1 - h));
>> grad = (1 / m) * X' * (h - y);
>> 
>> % Set options for fminunc
>> options = optimset('GradObj', 'on', 'MaxIter', 400);
>> % Run fminunc to obtain the optimal theta>> % This function will return theta and the cost
>> [theta, cost] = ...
> fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);
>> plotDecisionBoundary(theta, X, y);
>> hold on;
>> xlabel('Exam 1 score')
>> ylabel('Exam 2 score')
>> hold off;
>> 
>> prob = sigmoid([1 45 85] * theta);
>> p = predict(theta, X);
>> 
>> ex2
warning: broken pipe
warning: broken pipe

Plotting data with + indicating (y = 1) examples and o indicating (y = 0) examples.

Program paused. Press enter to continue.
Cost at initial theta (zeros): 0.693147
Expected cost (approx): 0.693
Gradient at initial theta (zeros): 
 -0.100000 
 -12.009217 
 -11.262842 
Expected gradients (approx):
 -0.1000
 -12.0092
 -11.2628

Cost at test theta: 0.218330
Expected cost (approx): 0.218
Gradient at test theta: 
 0.042903 
 2.566234 
 2.646797 
Expected gradients (approx):
 0.043
 2.566
 2.647

Program paused. Press enter to continue.
Cost at theta found by fminunc: 0.203498
Expected cost (approx): 0.203
theta: 
 -25.161272 
 0.206233 
 0.201470 
Expected theta (approx):
 -25.161
 0.206
 0.201

Program paused. Press enter to continue.
For a student with scores 45 and 85, we predict an admission probability of 0.776289
Expected value: 0.775 +/- 0.002

Train Accuracy: 89.000000
Expected accuracy (approx): 89.0

>> 
>> 
>> 
>> ex2_reg

Cost at initial theta (zeros): 0.693147
Expected cost (approx): 0.693
Gradient at initial theta (zeros) - first five values only:
 0.008475 
 0.018788 
 0.000078 
 0.050345 
 0.011501 
Expected gradients (approx) - first five values only:
 0.0085
 0.0188
 0.0001
 0.0503
 0.0115

Program paused. Press enter to continue.

Cost at test theta (with lambda = 10): 3.164509
Expected cost (approx): 3.16
Gradient at test theta - first five values only:
 0.346045 
 0.161352 
 0.194796 
 0.226863 
 0.092186 
Expected gradients (approx) - first five values only:
 0.3460
 0.1614
 0.1948
 0.2269
 0.0922

Program paused. Press enter to continue.

Train Accuracy: 83.050847
Expected accuracy (with lambda = 1): 83.1 (approx)
>> 
>> 
>> 
>> submit()
== Submitting solutions | Logistic Regression...
Use token from last successful submission (U201412842@hust.edu.cn)? (Y/n): Y
warning: operator -: automatic broadcasting operation applied
warning: quotient: automatic broadcasting operation applied
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==                            Sigmoid Function |   5 /   5 | Nice work!
==                    Logistic Regression Cost |  30 /  30 | Nice work!
==                Logistic Regression Gradient |  30 /  30 | Nice work!
==                                     Predict |   5 /   5 | Nice work!
==        Regularized Logistic Regression Cost |  15 /  15 | Nice work!
==    Regularized Logistic Regression Gradient |  15 /  15 | Nice work!
==                                   --------------------------------
==                                             | 100 / 100 | 
== 



%%%   Multi-class Classification and Neural Networks
@ https://github.com/lintingbin2009/machine-learning-ex/

octave:1> PS1('>> ')
>> cd /Users/xiapengfei/Desktop/machine-learning-ex3/ex3
>> input_layer_size  = 400; 
>> num_labels = 10;
>> % Load saved matrices from file
>> load('ex3data1.mat'); % training data stored in arrays X, y
>> % The matrices X and y will now be in your Octave environment
>> m = size(X, 1);
>> rand_indices = randperm(m);
>> sel = X(rand_indices(1:100), :);
>> displayData(sel);
>> theta_t = [-2; -1; 1; 2];
>> X_t = [ones(5,1) reshape(1:15,5,3)/10];
>> y_t = ([1;0;1;0;1] >= 0.5);
>> lambda_t = 3;
>> [J grad] = lrCostFunction(theta_t, X_t, y_t, lambda_t);
>> lambda = 0.1;
>> [all_theta] = oneVsAll(X, y, num_labels, lambda);
Iteration   500 | Cost: 1.311466e-02
Iteration   500 | Cost: 5.080194e-02
Iteration   500 | Cost: 5.760169e-02
Iteration   500 | Cost: 3.306491e-02
Iteration   500 | Cost: 5.445617e-02
Iteration   500 | Cost: 1.825591e-02
Iteration   500 | Cost: 3.064138e-02
Iteration   500 | Cost: 7.844983e-02
Iteration   500 | Cost: 7.118639e-02
Iteration   485 | Cost: 8.567845e-03
>> pred = predictOneVsAll(all_theta, X);
>> ex3


Loading and Visualizing Data ...
Program paused. Press enter to continue.

Testing lrCostFunction() with regularization
Cost: 2.534819
Expected cost: 2.534819
Gradients:
 0.146561 
 -0.548558 
 0.724722 
 1.398003 
Expected gradients:
 0.146561
 -0.548558
 0.724722
 1.398003
Program paused. Press enter to continue.

Training One-vs-All Logistic Regression...
Iteration   500 | Cost: 1.311466e-02
Iteration   500 | Cost: 5.080194e-02
Iteration   500 | Cost: 5.760169e-02
Iteration   500 | Cost: 3.306491e-02
Iteration   500 | Cost: 5.445617e-02
Iteration   500 | Cost: 1.825591e-02
Iteration   500 | Cost: 3.064138e-02
Iteration   500 | Cost: 7.844983e-02
Iteration   500 | Cost: 7.118639e-02
Iteration   485 | Cost: 8.567845e-03
Program paused. Press enter to continue.


Training Set Accuracy: 96.460000
>> 
>> 
>>
>> input_layer_size  = 400;
>> hidden_layer_size = 25;
>> num_labels = 10; 
>> load('ex3data1.mat');
>> m = size(X, 1);
>> sel = randperm(size(X, 1));
>> sel = sel(1:100);
>> displayData(X(sel, :));
>> load('ex3weights.mat');
>> pred = predict(Theta1, Theta2, X);
>> rp = randperm(m);
>> for i = 1:m
>     displayData(X(rp(i), :));
>     pred = predict(Theta1, Theta2, X(rp(i),:));
>     s = input('Paused - press enter to continue, q to exit:','s');
>     if s == 'q'
>       break
>     end
> end
Paused - press enter to continue, q to exit:q
>> ex3_nn

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Training Set Accuracy: 97.520000
Program paused. Press enter to continue.

Displaying Example Image

Neural Network Prediction: 9 (digit 9)
Paused - press enter to continue, q to exit:q
>> 
>> 
>> 
>> submit()
== Submitting solutions | Multi-class Classification and Neural Networks...
Use token from last successful submission (U201412842@hust.edu.cn)? (Y/n): Y
warning: operator -: automatic broadcasting operation applied
warning: quotient: automatic broadcasting operation applied
Iteration    26 | Cost: 3.391340e-01
Iteration    29 | Cost: 6.631438e-02
Iteration    30 | Cost: 1.002636e-01
Iteration    22 | Cost: 3.598119e-01
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==             Regularized Logistic Regression |  30 /  30 | Nice work!
==              One-vs-All Classifier Training |  20 /  20 | Nice work!
==            One-vs-All Classifier Prediction |  20 /  20 | Nice work!
==          Neural Network Prediction Function |  30 /  30 | Nice work!
==                                   --------------------------------
==                                             | 100 / 100 | 
== 
>> 